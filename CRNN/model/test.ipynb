{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 32, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : visual feature [batch_size x T x input_size]\n",
    "        output : contextual feature [batch_size x T x output_size]\n",
    "        \"\"\"\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
    "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
    "        return output\n",
    "\n",
    "#from rnn import BidirectionalLSTM\n",
    "\n",
    "rnn=nn.Sequential(\n",
    "        BidirectionalLSTM(512, 256, 256),\n",
    "        BidirectionalLSTM(256, 256, 256)\n",
    "        )\n",
    "\n",
    "out=torch.Tensor(32, 512, 1,70)\n",
    "out=out.squeeze(2).permute(2, 0, 1)\n",
    "out.shape\n",
    "# out =rnn(out)\n",
    "# out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 32, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=torch.Tensor(32, 512, 2,140)\n",
    "AdaptiveAvgPool =nn.AdaptiveAvgPool2d((1,70))\n",
    "out=AdaptiveAvgPool(out).permute(3, 0, 1, 2).squeeze(3)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=torch.tensor([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5],\n",
    "        [5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "         5, 5, 5]])\n",
    "y_hat.size(0)\n",
    "\n",
    "     \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '151', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '11', '1', '1', '1', '14151', '1', '1', '1', '1', '1', '141']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "character=['[GO]', '[s]', '[PAD]', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "# for index in enumerate(length):\n",
    "#     print(index.)\n",
    "#     for i in text_index[:, index] :\n",
    "#         print(i)\n",
    "#             # text = ''.join([character[i]] )\n",
    "#             # texts.append(text)\n",
    "def decode(t, length, raw=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param t: 预测中文的列表\n",
    "        :param length: 中文的长度\n",
    "        :param raw: False的话就直接转换(含有@)，否则返回不含-的中文\n",
    "        :return: 中文的字符串\n",
    "        \"\"\"\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length\n",
    "            if raw:\n",
    "                return ''.join([character[i - 1] for i in t])\n",
    "            else:\n",
    "                chinese_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        chinese_list.append(character[t[i] - 1])\n",
    "                return ''.join(chinese_list)\n",
    "        else:\n",
    "            assert t.numel() == length.sum()\n",
    "            texts = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                texts.append(\n",
    "                    decode(t[index:index + l], torch.IntTensor([l]), raw=raw)\n",
    "                )\n",
    "                index += l\n",
    "            return texts\n",
    "\n",
    "length=torch.tensor([27], dtype=torch.int32)\n",
    "y_hat = [decode(i, torch.IntTensor([y_hat.size(1)])) for i in y_hat]\n",
    "s=0\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdebb4970db76957bcf11e05672910c0fd8b516a13077c2765e7d5e9fe92ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
