{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from model.cabm import CBAM\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size,bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.rnn = nn.GRUCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.processed_batches = 0\n",
    "\n",
    "    def forward(self, prev_hidden, feats):\n",
    "        self.processed_batches = self.processed_batches + 1\n",
    "        nC = feats.size(0)\n",
    "        nB = feats.size(1)\n",
    "        nT = feats.size(2)\n",
    "        hidden_size = self.hidden_size\n",
    "        input_size = self.input_size\n",
    "\n",
    "        feats_proj = self.i2h(feats.view(-1,nC))\n",
    "        prev_hidden_proj = self.h2h(prev_hidden).view(1,nB, hidden_size).expand(nT, nB, hidden_size).contiguous().view(-1, hidden_size)\n",
    "        emition = self.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nT,nB).transpose(0,1)\n",
    "        alpha = F.softmax(emition, dim=1) # nB * nT\n",
    "\n",
    "        if self.processed_batches % 10000 == 0:\n",
    "            print('emition ', list(emition.data[0]))\n",
    "            print('alpha ', list(alpha.data[0]))\n",
    "\n",
    "        feats=feats.transpose(0, 2)\n",
    "        context = (feats * alpha.transpose(0,1).contiguous().view(nT,nB,1).expand(nT, nB, nC)).sum(0).squeeze(0)\n",
    "        cur_hidden = self.rnn(context, prev_hidden)\n",
    "        return cur_hidden, alpha\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_cell = AttentionCell(input_size, hidden_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.generator = nn.Linear(hidden_size, num_classes)\n",
    "        self.processed_batches = 0\n",
    "\n",
    "    def forward(self, feats, text_length):\n",
    "        self.processed_batches = self.processed_batches + 1\n",
    "        nC = feats.size(0)\n",
    "        nB = feats.size(1)\n",
    "        nT = feats.size(2)\n",
    "        hidden_size = self.hidden_size\n",
    "        input_size = self.input_size\n",
    "        print(\"in=\", input_size)\n",
    "        assert(input_size == nC)\n",
    "        assert(nB == text_length.numel())\n",
    "\n",
    "        num_steps = text_length.data.max()\n",
    "        num_labels = text_length.data.sum()\n",
    "\n",
    "        output_hiddens = Variable(torch.zeros(num_steps, nB, hidden_size).type_as(feats.data))\n",
    "        hidden = Variable(torch.zeros(nB,hidden_size).type_as(feats.data))\n",
    "        max_locs = torch.zeros(num_steps, nB)\n",
    "        max_vals = torch.zeros(num_steps, nB)\n",
    "        for i in range(num_steps):\n",
    "            hidden, alpha = self.attention_cell(hidden, feats)\n",
    "            output_hiddens[i] = hidden\n",
    "            if self.processed_batches % 500 == 0:\n",
    "                max_val, max_loc = alpha.data.max(1)\n",
    "                max_locs[i] = max_loc.cpu()\n",
    "                max_vals[i] = max_val.cpu()\n",
    "        if self.processed_batches % 500 == 0:\n",
    "            print('max_locs', list(max_locs[0:text_length.data[0],0]))\n",
    "            print('max_vals', list(max_vals[0:text_length.data[0],0]))\n",
    "        new_hiddens = Variable(torch.zeros(num_labels, hidden_size).type_as(feats.data))\n",
    "        b = 0\n",
    "        start = 0\n",
    "        for length in text_length.data:\n",
    "            new_hiddens[start:start+length] = output_hiddens[0:length,b,:]\n",
    "            start = start + length\n",
    "            b = b + 1\n",
    "        probs = self.generator(new_hiddens)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "at=AttentionCell(251, 251)\n",
    "xt=Attention(251, 251, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 10 11\n",
      "feats_proj.shape= torch.Size([110, 251])\n",
      "prev_hidden_proj = torch.Size([110, 251])\n",
      "x= torch.Size([110, 251])\n",
      "x= torch.Size([110, 1])\n",
      "x= torch.Size([11, 10])\n",
      "enmition= torch.Size([10, 11])\n"
     ]
    }
   ],
   "source": [
    "prev_hidden=torch.rand(10, 251)\n",
    "hidden_size=251\n",
    "feats=torch.rand(251, 10, 11)\n",
    "nT = feats.size(0)\n",
    "nB = feats.size(1)\n",
    "nC = feats.size(2)\n",
    "print(nT, nB, nC)\n",
    "feats_proj = at.i2h(feats.view(-1, nT))\n",
    "print(\"feats_proj.shape=\", feats_proj.shape)\n",
    "y=at.h2h(prev_hidden)\n",
    "print\n",
    "prev_hidden_proj = at.h2h(prev_hidden).view(1,nB, hidden_size).expand(nC, nB, hidden_size).contiguous().view(-1, hidden_size)\n",
    "print(\"prev_hidden_proj =\", prev_hidden_proj.shape)\n",
    "x=torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)\n",
    "print('x=' , x.shape)\n",
    "x=at.score(x)\n",
    "print('x=' , x.shape)\n",
    "x=x .view(nC,nB)\n",
    "print('x=' , x.shape)\n",
    "emition = at.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nC,nB).transpose(0,1)\n",
    "print(\"enmition=\", emition.shape)\n",
    "# alpha = F.softmax(emition, dim=1)\n",
    "# print('alpha=', alpha.shape)\n",
    "# # alpha =alpha.transpose(0, 1)\n",
    "# alpha=alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT)\n",
    "# print(alpha.shape)\n",
    "# feats=feats.transpose(0, 2)\n",
    "# print(feats.shape)\n",
    "# x=feats*alpha\n",
    "# x=x.sum(0).squeeze(0)\n",
    "# x.shape\n",
    "# #(feats*alpha).sum(0).squeeze(0).shape\n",
    "# #feats*(alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT))\n",
    "# #context = (feats * alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT)).sum(0).squeeze(0)\n",
    "# cur_hidden = at.rnn(x, prev_hidden)\n",
    "# cur_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in= 251\n",
      "(num_labels.shape= torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "feats=torch.rand(251, 10, 11)\n",
    "nC = feats.size(0)\n",
    "nB = feats.size(1)\n",
    "nT = feats.size(2)\n",
    "text_length=torch.tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "hidden_size = xt.hidden_size\n",
    "input_size = xt.input_size\n",
    "print(\"in=\", input_size)\n",
    "assert(input_size == nC)\n",
    "assert(nB == text_length.numel())\n",
    "num_steps = text_length.data.max()\n",
    "num_labels = text_length.data.sum()\n",
    "print('(num_labels.shape=', num_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 8, 11])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # 车牌中可能出现的所有词，包括blank标签-，其中blank标签的索引为0\n",
    "#     ALL_CLASSES = ['-','京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
    "#              '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
    "#              '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
    "#              '新',\n",
    "#              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "#              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
    "#              'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "#              'W', 'X', 'Y', 'Z', 'I', 'O']\n",
    "\n",
    "#     # 假如batch size为4，即一个batch中有4个车牌\n",
    "#     batch_target_groudtruth = [\n",
    "#         ['湘','E','2','6','9','J','Y'],\n",
    "#         ['冀','P','L','3','N','6','7'],\n",
    "#         ['川','R','6','7','2','8','3','F'],\n",
    "#         ['津','A','D','6','8','4','2','9']\n",
    "#     ]\n",
    "\n",
    "#     # 根据车牌的文字信息获取当前batch中四个车牌的标签\n",
    "#     target_label = []\n",
    "#     for license_palte in batch_target_groudtruth:\n",
    "#         temp_list = []\n",
    "#         for x in license_palte:\n",
    "#             for i,classes in enumerate(ALL_CLASSES):\n",
    "#                 if x == classes:\n",
    "#                     temp_list.append(i)\n",
    "#         target_label.append(temp_list)\n",
    "\n",
    "#     print(target_label)\n",
    "\n",
    "#     # 获取四个车牌每个车牌的长度\n",
    "#     target_lengths_list = [len(label) for label in target_label]\n",
    "\n",
    "\n",
    "#     T = 50  # 输入序列长度\n",
    "#     C = len(ALL_CLASSES)  # 分类总数量，包括blank\n",
    "#     N = 4  # Batch Size\n",
    "#     print('c=',C)\n",
    "#     # 初始化一个随机输入序列，形状为(T,N,C)=>(50,4,20)\n",
    "#     input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "#     print('input.shape=',input.shape)\n",
    "#     # 初始化输入序列长度Tensor，形状为N，值为T\n",
    "#     input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "#     print('input_length=',input_lengths.shape)\n",
    "#     # 以四个车牌的长度7,7,8,8作为目标序列长度\n",
    "#     target_lengths = torch.tensor(target_lengths_list,dtype=torch.long)\n",
    "#     print('target_lengths=',target_lengths.shape)\n",
    "#     # 以四个车牌的lable作为目标序列，采用sum(target_lengths)形式，形状为30的一维tensor\n",
    "#     target = torch.tensor(list(itertools.chain.from_iterable(target_label)),dtype=torch.long)\n",
    "#     print(target.shape)\n",
    "#     # 创建一个CTCLoss对象\n",
    "#     ctc_loss = torch.nn.CTCLoss(blank=0)\n",
    "\n",
    "#     # 调用CTCLoss()对象计算损失值\n",
    "#     loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "    \n",
    "log_probs = torch.randn(27,8,11).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "log_probs.shape\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Xiaoyao')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdebb4970db76957bcf11e05672910c0fd8b516a13077c2765e7d5e9fe92ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
