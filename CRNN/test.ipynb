{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from model.cabm import CBAM\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size,bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.rnn = nn.GRUCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.processed_batches = 0\n",
    "\n",
    "    def forward(self, prev_hidden, feats):\n",
    "        self.processed_batches = self.processed_batches + 1\n",
    "        nC = feats.size(0)\n",
    "        nB = feats.size(1)\n",
    "        nT = feats.size(2)\n",
    "        hidden_size = self.hidden_size\n",
    "        input_size = self.input_size\n",
    "\n",
    "        feats_proj = self.i2h(feats.view(-1,nC))\n",
    "        prev_hidden_proj = self.h2h(prev_hidden).view(1,nB, hidden_size).expand(nT, nB, hidden_size).contiguous().view(-1, hidden_size)\n",
    "        emition = self.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nT,nB).transpose(0,1)\n",
    "        alpha = F.softmax(emition, dim=1) # nB * nT\n",
    "\n",
    "        if self.processed_batches % 10000 == 0:\n",
    "            print('emition ', list(emition.data[0]))\n",
    "            print('alpha ', list(alpha.data[0]))\n",
    "\n",
    "        feats=feats.transpose(0, 2)\n",
    "        context = (feats * alpha.transpose(0,1).contiguous().view(nT,nB,1).expand(nT, nB, nC)).sum(0).squeeze(0)\n",
    "        cur_hidden = self.rnn(context, prev_hidden)\n",
    "        return cur_hidden, alpha\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_cell = AttentionCell(input_size, hidden_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.generator = nn.Linear(hidden_size, num_classes)\n",
    "        self.processed_batches = 0\n",
    "\n",
    "    def forward(self, feats, text_length):\n",
    "        self.processed_batches = self.processed_batches + 1\n",
    "        nC = feats.size(0)\n",
    "        nB = feats.size(1)\n",
    "        nT = feats.size(2)\n",
    "        hidden_size = self.hidden_size\n",
    "        input_size = self.input_size\n",
    "        print(\"in=\", input_size)\n",
    "        assert(input_size == nC)\n",
    "        assert(nB == text_length.numel())\n",
    "\n",
    "        num_steps = text_length.data.max()\n",
    "        num_labels = text_length.data.sum()\n",
    "\n",
    "        output_hiddens = Variable(torch.zeros(num_steps, nB, hidden_size).type_as(feats.data))\n",
    "        hidden = Variable(torch.zeros(nB,hidden_size).type_as(feats.data))\n",
    "        max_locs = torch.zeros(num_steps, nB)\n",
    "        max_vals = torch.zeros(num_steps, nB)\n",
    "        for i in range(num_steps):\n",
    "            hidden, alpha = self.attention_cell(hidden, feats)\n",
    "            output_hiddens[i] = hidden\n",
    "            if self.processed_batches % 500 == 0:\n",
    "                max_val, max_loc = alpha.data.max(1)\n",
    "                max_locs[i] = max_loc.cpu()\n",
    "                max_vals[i] = max_val.cpu()\n",
    "        if self.processed_batches % 500 == 0:\n",
    "            print('max_locs', list(max_locs[0:text_length.data[0],0]))\n",
    "            print('max_vals', list(max_vals[0:text_length.data[0],0]))\n",
    "        new_hiddens = Variable(torch.zeros(num_labels, hidden_size).type_as(feats.data))\n",
    "        b = 0\n",
    "        start = 0\n",
    "        for length in text_length.data:\n",
    "            new_hiddens[start:start+length] = output_hiddens[0:length,b,:]\n",
    "            start = start + length\n",
    "            b = b + 1\n",
    "        probs = self.generator(new_hiddens)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "at=AttentionCell(251, 251)\n",
    "xt=Attention(251, 251, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 10 11\n",
      "feats_proj.shape= torch.Size([110, 251])\n",
      "prev_hidden_proj = torch.Size([110, 251])\n",
      "x= torch.Size([110, 251])\n",
      "x= torch.Size([110, 1])\n",
      "x= torch.Size([11, 10])\n",
      "enmition= torch.Size([10, 11])\n"
     ]
    }
   ],
   "source": [
    "prev_hidden=torch.rand(10, 251)\n",
    "hidden_size=251\n",
    "feats=torch.rand(251, 10, 11)\n",
    "nT = feats.size(0)\n",
    "nB = feats.size(1)\n",
    "nC = feats.size(2)\n",
    "print(nT, nB, nC)\n",
    "feats_proj = at.i2h(feats.view(-1, nT))\n",
    "print(\"feats_proj.shape=\", feats_proj.shape)\n",
    "y=at.h2h(prev_hidden)\n",
    "print\n",
    "prev_hidden_proj = at.h2h(prev_hidden).view(1,nB, hidden_size).expand(nC, nB, hidden_size).contiguous().view(-1, hidden_size)\n",
    "print(\"prev_hidden_proj =\", prev_hidden_proj.shape)\n",
    "x=torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)\n",
    "print('x=' , x.shape)\n",
    "x=at.score(x)\n",
    "print('x=' , x.shape)\n",
    "x=x .view(nC,nB)\n",
    "print('x=' , x.shape)\n",
    "emition = at.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nC,nB).transpose(0,1)\n",
    "print(\"enmition=\", emition.shape)\n",
    "# alpha = F.softmax(emition, dim=1)\n",
    "# print('alpha=', alpha.shape)\n",
    "# # alpha =alpha.transpose(0, 1)\n",
    "# alpha=alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT)\n",
    "# print(alpha.shape)\n",
    "# feats=feats.transpose(0, 2)\n",
    "# print(feats.shape)\n",
    "# x=feats*alpha\n",
    "# x=x.sum(0).squeeze(0)\n",
    "# x.shape\n",
    "# #(feats*alpha).sum(0).squeeze(0).shape\n",
    "# #feats*(alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT))\n",
    "# #context = (feats * alpha.transpose(0,1).contiguous().view(nC,nB,1).expand(nC, nB, nT)).sum(0).squeeze(0)\n",
    "# cur_hidden = at.rnn(x, prev_hidden)\n",
    "# cur_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in= 251\n",
      "(num_labels.shape= torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "feats=torch.rand(251, 10, 11)\n",
    "nC = feats.size(0)\n",
    "nB = feats.size(1)\n",
    "nT = feats.size(2)\n",
    "text_length=torch.tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "hidden_size = xt.hidden_size\n",
    "input_size = xt.input_size\n",
    "print(\"in=\", input_size)\n",
    "assert(input_size == nC)\n",
    "assert(nB == text_length.numel())\n",
    "num_steps = text_length.data.max()\n",
    "num_labels = text_length.data.sum()\n",
    "print('(num_labels.shape=', num_labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Xiaoyao')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdebb4970db76957bcf11e05672910c0fd8b516a13077c2765e7d5e9fe92ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
